{"metadata":{"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"#Step 1- Import the text file/article that has to be used for MCQ generation\n\nfile=open(\"article.txt\",\"r\") #\"r\" deontes read version open\ntext=file.read()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('popular')\n#Importing the needed files and packages","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Step 2- Extract the important words(keywords) from the text article that can be used to create MCQ using PKE (Python Keyword Extraction)\n\nimport pke\nfrom nltk.corpus import stopwords #Stopwords are the words that we need to avoid while considering keyword extraction\nimport string\ndef getImportantWords(art): \n    extractor=pke.unsupervised.MultipartiteRank() #Using the Multipartite Unsupervised Extractor as our extractor\n    extractor.load_document(input=art,language='en')\n    pos={'PROPN'} #We are only considering proper nouns as valid candidates for our keywords\n    stops=list(string.punctuation) #Stoplist contains the words to be avoided\n    stops+=['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-'] #These stand for the brackets as in lrb=left round bracket=\"(\" and so on\n    stops+=stopwords.words('english')\n    extractor.candidate_selection(pos=pos,stoplist=stops) #Sets the candidate selection criteria, as in, which should be considered and which should be avoided\n    extractor.candidate_weighting() #Sets the preference criteria for the candidates\n    result=[] \n    ex=extractor.get_n_best(n=10) #Gets the 10 best candidates according to the criteria set\n    for each in ex:\n        result.append(each[0]) \n    return result\nimpWords=getImportantWords(text) #Get the important words (keywords) from the text article using the above function\n#print(impWords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Step 3- Split the whole text article into an array/list of individual sentences. This will help us fetch the sentences related to the keywords easily\n\nfrom nltk.tokenize import sent_tokenize\ndef splitTextToSents(art):\n    s=[sent_tokenize(art)]\n    s=[y for x in s for y in x]\n    s=[sent.strip() for sent in s if len(sent)>15] #Removes all the sentences that have length less than 15 so that we can ensure that our questions have enough length for context\n    return s\nsents=splitTextToSents(text) #Achieve a well splitted set of sentences from the text article\n#print(sents)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Step 4- Map the sentences which contain the keywords to the related keywords so that we can easily lookup the sentences related to the keywords\n\nfrom flashtext import KeywordProcessor\ndef mapSents(impWords,sents):\n    processor=KeywordProcessor() #Using keyword processor as our processor for this task\n    keySents={}\n    for word in impWords:\n        keySents[word]=[]\n        processor.add_keyword(word) #Adds key word to the processor\n    for sent in sents:\n        found=processor.extract_keywords(sent) #Extract the keywords in the sentence\n        for each in found:\n            keySents[each].append(sent) #For each keyword found, map the sentence to the keyword\n    for key in keySents.keys():\n        temp=keySents[key]\n        temp=sorted(temp,key=len,reverse=True) #Sort the sentences according to their decreasing length in order to ensure the quality of question for the MCQ \n        keySents[key]=temp\n    return keySents\nmappedSents=mapSents(impWords,sents) #Achieve the sentences that contain the keywords and map those sentences to the keywords using this function\n#print(mappedSents)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Step 5- Get the sense of the word. In order to attain a quality set of distractors we need to get the right sense of the keyword. This is explained in detail in the seperate alogrithm documentation\n\nfrom pywsd.similarity import max_similarity\nfrom pywsd.lesk import adapted_lesk\nfrom pywsd.lesk import simple_lesk\nfrom pywsd.lesk import cosine_lesk\nfrom nltk.corpus import wordnet as wn\ndef getWordSense(sent,word):\n    word=word.lower() \n    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n        word=word.replace(\" \",\"_\")\n    synsets=wn.synsets(word,'n') #Sysnets from Google are invoked\n    if synsets:\n        wup=max_similarity(sent,word,'wup',pos='n')\n        adapted_lesk_output = adapted_lesk(sent, word, pos='n')\n        lowest_index=min(synsets.index(wup),synsets.index(adapted_lesk_output))\n        return synsets[lowest_index]\n    else:\n        return None\n#print(\"fin\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Step 6- Get distractor from WordNet. These distractors work on the basis of hypernym and hyponym explained in detail in the documentation.\n\ndef getDistractors(syn,word):\n    dists=[]\n    word=word.lower()\n    actword=word\n    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n        word.replace(\" \",\"_\")\n    hypernym = syn.hypernyms() #Gets the hypernyms of the word\n    if len(hypernym)==0: #If there are no hypernyms for the current word, we simple return the empty list of distractors\n        return dists\n    for each in hypernym[0].hyponyms(): #Other wise we find the relevant hyponyms for the hypernyms\n        name=each.lemmas()[0].name()\n        if(name==actword):\n            continue\n        name=name.replace(\"_\",\" \")\n        name=\" \".join(w.capitalize() for w in name.split())\n        if name is not None and name not in dists: #If the word is not already present in the list and is different from he actial word\n            dists.append(name)\n    return dists\n#print(\"fin\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Step 7- The primary goal of this step is to take our MCQ quality one step further. The WordNet might some times fail to produce a hypernym for some words.\n#In that case the ConcepNet comes into play as they help achieve our distractors when there are no hypernyms present for it in the WordNet. More about this is discussed\n#in the algorithm documentation.\n\nimport requests\nimport json\ndef getDistractors2(word):\n    word=word.lower()\n    actword=word\n    if len(word.split())>0: #Splits the word with underscores(_) instead of spaces if there are multiple words\n        word=word.replace(\" \",\"_\")\n    dists=[]\n    url= \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word) #To get ditractors from ConceptNet's API\n    obj=requests.get(url).json()\n    for edge in obj['edges']:\n        link=edge['end']['term']\n        url2=\"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n        obj2=requests.get(url2).json()\n        for edge in obj2['edges']:\n            word2=edge['start']['label']\n            if word2 not in dists and actword.lower() not in word2.lower(): #If the word is not already present in the list and is different from he actial word\n                dists.append(word2)\n    return dists\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Step 8- Find and map the distractors to the keywords\n\nmappedDists={}\nfor each in mappedSents:\n    wordsense=getWordSense(mappedSents[each][0],each) #gets the sense of the word\n    if wordsense: #if the wordsense is not null/none\n        dists=getDistractors(wordsense,each) #Gets the WordNet distractors\n        if len(dists)==0: #If there are no WordNet distractors available for the current word\n            dists=getDistractors2(each) #The gets the distractors from the ConceptNet API\n        if len(dists)!=0: #If there are indeed distractors from WordNet available, then maps them\n            mappedDists[each]=dists\n    else: #If there is no wordsense, the directly searches/uses the ConceptNet\n        dists=getDistractors2(each)\n        if len(dists)>0: #If it gets the Distractors then maps them\n            mappedDists[each]=dists\n#print(mappedDists)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Step 9- The final step is to present our MCQ in a nice and readable manner.\n\nprint(\"**************************************        Multiple Choice Questions        *******************************\")\nprint()\nimport re\nimport random\niterator = 1 #To keep the count of the questions\nfor each in mappedDists:\n    sent=mappedSents[each][0]\n    p=re.compile(each,re.IGNORECASE) #Converts into regular expression for pattern matching\n    op=p.sub(\"________\",sent) #Replaces the keyword with underscores(blanks)\n    print(\"Question %s-> \"%(iterator),op) #Prints the question along with a question number\n    options=[each.capitalize()]+mappedDists[each] #Capitalizes the options\n    options=options[:4] #Selects only 4 options\n    opts=['a','b','c','d']\n    random.shuffle(options) #Shuffle the options so that order is not always same\n    for i,ch in enumerate(options):\n        print(\"\\t\",opts[i],\") \", ch) #Print the options\n    print()\n    iterator+=1 #Increase the counter","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]}]}