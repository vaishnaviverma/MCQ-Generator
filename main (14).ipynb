{"metadata":{"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"file=open(\"article.txt\",\"r\")\ntext=file.read()\n#print(text)","metadata":{"trusted":true},"execution_count":21,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('popular')","metadata":{"trusted":true},"execution_count":37,"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /home/jovyan/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n[nltk_data] Downloading package punkt to /home/jovyan/nltk_data...\n[nltk_data]   Package punkt is already up-to-date!\n[nltk_data] Downloading collection 'popular'\n[nltk_data]    | \n[nltk_data]    | Downloading package cmudict to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping corpora/cmudict.zip.\n[nltk_data]    | Downloading package gazetteers to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n[nltk_data]    | Downloading package genesis to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping corpora/genesis.zip.\n[nltk_data]    | Downloading package gutenberg to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n[nltk_data]    | Downloading package inaugural to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping corpora/inaugural.zip.\n[nltk_data]    | Downloading package movie_reviews to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n[nltk_data]    | Downloading package names to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping corpora/names.zip.\n[nltk_data]    | Downloading package shakespeare to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n[nltk_data]    | Downloading package stopwords to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Package stopwords is already up-to-date!\n[nltk_data]    | Downloading package treebank to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping corpora/treebank.zip.\n[nltk_data]    | Downloading package twitter_samples to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n[nltk_data]    | Downloading package omw to /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping corpora/omw.zip.\n[nltk_data]    | Downloading package wordnet to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping corpora/wordnet.zip.\n[nltk_data]    | Downloading package wordnet_ic to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n[nltk_data]    | Downloading package words to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping corpora/words.zip.\n[nltk_data]    | Downloading package maxent_ne_chunker to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n[nltk_data]    | Downloading package punkt to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Package punkt is already up-to-date!\n[nltk_data]    | Downloading package snowball_data to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    | Downloading package averaged_perceptron_tagger to\n[nltk_data]    |     /home/jovyan/nltk_data...\n[nltk_data]    |   Unzipping taggers/averaged_perceptron_tagger.zip.\n[nltk_data]    | \n[nltk_data]  Done downloading collection popular\n","output_type":"stream"},{"execution_count":37,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import itertools\nimport pke\nfrom nltk.corpus import stopwords\nimport string\n#print(\"done\")","metadata":{"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"done\n","output_type":"stream"}]},{"cell_type":"code","source":"def getImportantWords(art):\n    extractor=pke.unsupervised.MultipartiteRank()\n    extractor.load_document(input=art,language='en')\n    pos={'PROPN'}\n    stops=list(string.punctuation)\n    stops+=['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n    stops+=stopwords.words('english')\n    extractor.candidate_selection(pos=pos,stoplist=stops)\n    extractor.candidate_weighting()\n    result=[]\n    ex=extractor.get_n_best(n=10)\n    for each in ex:\n        result.append(each[0])\n    return result\nimpWords=getImportantWords(text)\nprint(impWords)","metadata":{"trusted":true},"execution_count":26,"outputs":[{"name":"stdout","text":"['egyptians', 'nile river', 'egypt', 'nile', 'euphrates', 'tigris', 'old kingdom', 'red land', 'pharaoh', 'double crown']\n","output_type":"stream"}]},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\ndef splitTextToSents(art):\n    s=[sent_tokenize(art)]\n    s=[y for x in s for y in x]\n    s=[sent.strip() for sent in s if len(sent)>15]\n    return s\nsents=splitTextToSents(text)\n#print(sents)","metadata":{"trusted":true},"execution_count":34,"outputs":[]},{"cell_type":"code","source":"from flashtext import KeywordProcessor\ndef mapSents(impWords,sents):\n    processor=KeywordProcessor()\n    keySents={}\n    for word in impWords:\n        keySents[word]=[]\n        processor.add_keyword(word)\n    for sent in sents:\n        found=processor.extract_keywords(sent)\n        for each in found:\n            keySents[each].append(sent)\n    for key in keySents.keys():\n        temp=keySents[key]\n        temp=sorted(temp,key=len,reverse=True)\n        keySents[key]=temp\n    return keySents\nmappedSents=mapSents(impWords,sents)\n#print(mappedSents)","metadata":{"trusted":true},"execution_count":59,"outputs":[]},{"cell_type":"code","source":"from pywsd.similarity import max_similarity\nfrom pywsd.lesk import adapted_lesk\nfrom pywsd.lesk import simple_lesk\nfrom pywsd.lesk import cosine_lesk\nfrom nltk.corpus import wordnet as wn\ndef getWordSense(sent,word):\n    word=word.lower()\n    if len(word.split())>0:\n        word=word.replace(\" \",\"_\")\n    synsets=wn.synsets(word,'n')\n    if synsets:\n        wup=max_similarity(sent,word,'wup',pos='n')\n        adapted_lesk_output = adapted_lesk(sent, word, pos='n')\n        lowest_index=min(synsets.index(wup),synsets.index(adapted_lesk_output))\n        return synsets[lowest_index]\n    else:\n        return None\nprint(\"fin\")","metadata":{"trusted":true},"execution_count":58,"outputs":[{"name":"stdout","text":"fin\n","output_type":"stream"}]},{"cell_type":"code","source":"def getDistractors(syn,word):\n    dists=[]\n    word=word.lower()\n    actword=word\n    if len(word.split())>0:\n        word.replace(\" \",\"_\")\n    hypernym = syn.hypernyms()\n    if len(hypernym)==0:\n        return dists\n    for each in hypernym[0].hyponyms():\n        name=each.lemmas()[0].name()\n        if(name==actword):\n            continue\n        name=name.replace(\"_\",\" \")\n        name=\" \".join(w.capitalize() for w in name.split())\n        if name is not None and name not in dists:\n            dists.append(name)\n    return dists\nprint(\"fin\")","metadata":{"trusted":true},"execution_count":63,"outputs":[{"name":"stdout","text":"fin\n","output_type":"stream"}]},{"cell_type":"code","source":"import requests\nimport json\ndef getDistractors2(word):\n    word=word.lower()\n    actword=word\n    if len(word.split())>0:\n        word=word.replace(\" \",\"_\")\n    dists=[]\n    url= \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n    obj=requests.get(url).json()\n    for edge in obj['edges']:\n        link=edge['end']['term']\n        url2=\"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n        obj2=requests.get(url2).json()\n        for edge in obj2['edges']:\n            word2=edge['start']['label']\n            if word2 not in dists and actword.lower() not in word2.lower():\n                dists.append(word2)\n    return dists\n","metadata":{"trusted":true},"execution_count":75,"outputs":[]},{"cell_type":"code","source":"mappedDists={}\nfor each in mappedSents:\n    wordsense=getWordSense(mappedSents[each][0],each)\n    if wordsense:\n        dists=getDistractors(wordsense,each)\n        if len(dists)==0:\n            dists=getDistractors2(each)\n        if len(dists)!=0:\n            mappedDists[each]=dists\n    else:\n        dists=getDistractors2(each)\n        if len(dists)>0:\n            mappedDists[each]=dists\n#print(mappedDists)","metadata":{"trusted":true},"execution_count":77,"outputs":[]},{"cell_type":"code","source":"print(\"**************************************        Multiple Choice Questions        *******************************\")\nprint()\nimport re\niterator = 1\nfor each in mappedDists:\n    sent=mappedSents[each][0]\n    p=re.compile(each,re.IGNORECASE)\n    op=p.sub(\"________\",sent)\n    print(\"Question %s-> \"%(iterator),op)\n    options=[each.capitalize()]+mappedDists[each]\n    options=options[:4]\n    opts=['a','b','c','d']\n    random.shuffle(options)\n    for i,ch in enumerate(options):\n        print(\"\\t\",opts[i],\") \", ch)\n    print()\n    iterator+=1","metadata":{"trusted":true},"execution_count":85,"outputs":[{"name":"stdout","text":"**************************************        Multiple Choice Questions        *******************************\n\nQuestion 1->  ________ cut the stems into strips, pressed them, and dried them into sheets that could be rolled into scrolls.\n\t a )  Egyptians\n\t b )  Angolan\n\t c )  Bantu\n\t d )  Algerian\n\nQuestion 2->  As in many ancient societies, much of the knowledge of ________ came about as priests studied the world to find ways to please the gods.\n\t a )  Egypt\n\t b )  Saudi Arabia\n\t c )  Iraq\n\t d )  Kuwait\n\nQuestion 3->  About 5,000 years ago, they noticed that a star now called Sirius appeared shortly before the ________ began to flood.\n\t a )  Entebbe\n\t b )  Buganda\n\t c )  Nile\n\t d )  Gulu\n\nQuestion 4->  Unlike the Tigris and ________, the Nile River flooded at the same time every year, so farmers could predict when to plant their crops.\n\t a )  Adana\n\t b )  Euphrates\n\t c )  Edirne\n\t d )  Dardanelles\n\nQuestion 5->  Unlike the ________ and Euphrates, the Nile River flooded at the same time every year, so farmers could predict when to plant their crops.\n\t a )  Euphrates\n\t b )  Tigris\n\t c )  Dardanelles\n\t d )  Edirne\n\nQuestion 6->  About 80 years later, a ________ named Khufu decided he wanted a monument that would show the world how great he was.\n\t a )  Pharaoh\n\t b )  Bourbon\n\t c )  Caliph\n\t d )  Basileus\n\n","output_type":"stream"}]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}