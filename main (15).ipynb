{"metadata":{"language_info":{"name":"python","version":"3.7.8","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"file=open(\"article.txt\",\"r\")\ntext=file.read()\n#print(text)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import nltk\nnltk.download('stopwords')\nnltk.download('punkt')\nnltk.download('popular')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import itertools\nimport pke\nfrom nltk.corpus import stopwords\nimport string\n#print(\"fin\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getImportantWords(art):\n    extractor=pke.unsupervised.MultipartiteRank()\n    extractor.load_document(input=art,language='en')\n    pos={'PROPN'}\n    stops=list(string.punctuation)\n    stops+=['-lrb-', '-rrb-', '-lcb-', '-rcb-', '-lsb-', '-rsb-']\n    stops+=stopwords.words('english')\n    extractor.candidate_selection(pos=pos,stoplist=stops)\n    extractor.candidate_weighting()\n    result=[]\n    ex=extractor.get_n_best(n=10)\n    for each in ex:\n        result.append(each[0])\n    return result\nimpWords=getImportantWords(text)\nprint(impWords)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.tokenize import sent_tokenize\ndef splitTextToSents(art):\n    s=[sent_tokenize(art)]\n    s=[y for x in s for y in x]\n    s=[sent.strip() for sent in s if len(sent)>15]\n    return s\nsents=splitTextToSents(text)\n#print(sents)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from flashtext import KeywordProcessor\ndef mapSents(impWords,sents):\n    processor=KeywordProcessor()\n    keySents={}\n    for word in impWords:\n        keySents[word]=[]\n        processor.add_keyword(word)\n    for sent in sents:\n        found=processor.extract_keywords(sent)\n        for each in found:\n            keySents[each].append(sent)\n    for key in keySents.keys():\n        temp=keySents[key]\n        temp=sorted(temp,key=len,reverse=True)\n        keySents[key]=temp\n    return keySents\nmappedSents=mapSents(impWords,sents)\n#print(mappedSents)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from pywsd.similarity import max_similarity\nfrom pywsd.lesk import adapted_lesk\nfrom pywsd.lesk import simple_lesk\nfrom pywsd.lesk import cosine_lesk\nfrom nltk.corpus import wordnet as wn\ndef getWordSense(sent,word):\n    word=word.lower()\n    if len(word.split())>0:\n        word=word.replace(\" \",\"_\")\n    synsets=wn.synsets(word,'n')\n    if synsets:\n        wup=max_similarity(sent,word,'wup',pos='n')\n        adapted_lesk_output = adapted_lesk(sent, word, pos='n')\n        lowest_index=min(synsets.index(wup),synsets.index(adapted_lesk_output))\n        return synsets[lowest_index]\n    else:\n        return None\nprint(\"fin\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def getDistractors(syn,word):\n    dists=[]\n    word=word.lower()\n    actword=word\n    if len(word.split())>0:\n        word.replace(\" \",\"_\")\n    hypernym = syn.hypernyms()\n    if len(hypernym)==0:\n        return dists\n    for each in hypernym[0].hyponyms():\n        name=each.lemmas()[0].name()\n        if(name==actword):\n            continue\n        name=name.replace(\"_\",\" \")\n        name=\" \".join(w.capitalize() for w in name.split())\n        if name is not None and name not in dists:\n            dists.append(name)\n    return dists\nprint(\"fin\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import requests\nimport json\ndef getDistractors2(word):\n    word=word.lower()\n    actword=word\n    if len(word.split())>0:\n        word=word.replace(\" \",\"_\")\n    dists=[]\n    url= \"http://api.conceptnet.io/query?node=/c/en/%s/n&rel=/r/PartOf&start=/c/en/%s&limit=5\"%(word,word)\n    obj=requests.get(url).json()\n    for edge in obj['edges']:\n        link=edge['end']['term']\n        url2=\"http://api.conceptnet.io/query?node=%s&rel=/r/PartOf&end=%s&limit=10\"%(link,link)\n        obj2=requests.get(url2).json()\n        for edge in obj2['edges']:\n            word2=edge['start']['label']\n            if word2 not in dists and actword.lower() not in word2.lower():\n                dists.append(word2)\n    return dists\n","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"mappedDists={}\nfor each in mappedSents:\n    wordsense=getWordSense(mappedSents[each][0],each)\n    if wordsense:\n        dists=getDistractors(wordsense,each)\n        if len(dists)==0:\n            dists=getDistractors2(each)\n        if len(dists)!=0:\n            mappedDists[each]=dists\n    else:\n        dists=getDistractors2(each)\n        if len(dists)>0:\n            mappedDists[each]=dists\n#print(mappedDists)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(\"**************************************        Multiple Choice Questions        *******************************\")\nprint()\nimport re\niterator = 1\nfor each in mappedDists:\n    sent=mappedSents[each][0]\n    p=re.compile(each,re.IGNORECASE)\n    op=p.sub(\"________\",sent)\n    print(\"Question %s-> \"%(iterator),op)\n    options=[each.capitalize()]+mappedDists[each]\n    options=options[:4]\n    opts=['a','b','c','d']\n    random.shuffle(options)\n    for i,ch in enumerate(options):\n        print(\"\\t\",opts[i],\") \", ch)\n    print()\n    iterator+=1","metadata":{"scrolled":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}